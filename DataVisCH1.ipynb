{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tgarg535/Data-Visualization/blob/main/DataVisCH1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkawIrMkQG2k"
      },
      "source": [
        "THESE NOTES ARE REFERED FROM BOOK [link text](https://github.com/tgarg535/Data-Visualization/blob/d64d7b8accdf421c04189bda3e020d0b8e8ec245/Data%20Visualization%20Book.pdf)\n",
        "\n",
        "### **Chapter 1: Introduction**\n",
        "\n",
        "This chapter provides a high-level introduction to data visualization, covering its definition, historical context, relationship to other fields, the process of creating visualizations, and the crucial role of the user and human perception.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1.1 What Is Visualization?**\n",
        "\n",
        "**Definition**: Visualization is defined as \"the communication of information using graphical representations\". It leverages the human perceptual system's ability to process images in parallel, making it much faster than reading text, which is a sequential process. Visualizations can also be language-independent, like a map or graph understood by people who don't share a common language.\n",
        "\n",
        "**Visualization in Daily Life**: We encounter visualizations constantly, such as subway maps, weather charts, stock market graphs, 3D medical scans, and assembly instructions.\n",
        "\n",
        "**Why Visualization is Important**: The way data is presented can profoundly impact decision-making. Two key examples from the book highlight this:\n",
        "\n",
        "1.  **Data Distortion Through Scaling**: The same dataset can be perceived in dramatically different ways depending on how its axes are scaled. As seen in Figure 1.1 of the book, plotting car retail price vs. MPG with different scales can make the data appear as:\n",
        "    * A tight, undifferentiated cluster (when both axes have a large, uniform scale).\n",
        "    * A horizontal linear pattern (when the y-axis is scaled larger).\n",
        "    * A vertical linear pattern (when the x-axis is scaled larger).\n",
        "    * An inversely proportional curve (when the scale is determined by the data's range).\n",
        "    This shows that scaling can be used, intentionally or not, to distort the \"truthful\" representation of data.\n",
        "\n",
        "2.  **Human Interpretation of Different Formats**: A 1999 study by Elting et al. presented hypothetical clinical trial data to 34 clinicians using four different formats: a table, pie charts, stacked bar graphs, and an icon display.\n",
        "    * **Results**: The format had a massive impact on the clinicians' ability to make the correct decision to stop the trial.\n",
        "        * **Icon Displays**: 82% correct decisions.\n",
        "        * **Tables**: 68% correct decisions.\n",
        "        * **Pie Charts / Bar Graphs**: 56% correct decisions.\n",
        "    * **Crucial Insight**: This means that up to 25% of patients could have received inappropriate treatment based on data shown in bar or pie charts. Ironically, most clinicians preferred the table and were \"contemptuous of the icon display,\" demonstrating that user preference does not equate to effectiveness.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1.2 History of Visualization**\n",
        "\n",
        "The use of graphics to convey information has a long and rich history.\n",
        "\n",
        "* **Early Visualizations**:\n",
        "    * The earliest examples are **cave paintings**, such as those in the Chauvet Cave from approximately 30,000 years ago, used to record and pass on information.\n",
        "    * **Maps** were essential for travel, commerce, and religion.\n",
        "        * The **Peutinger Map** was an early Roman road map that distorted east-west distances to fit on a long scroll.\n",
        "        * The **Hereford Map** is a large medieval map of the world with Jerusalem at its center, mixing real geography with religious and mythical information.\n",
        "        * **John Snow's Cholera Map (1854)** is a famous example of thematic cartography. By plotting deaths on a map of London, he identified a strong cluster around the Broad Street water pump. He had the pump handle removed, ending the epidemic and demonstrating a clear geographical link to disease.\n",
        "    * **Time-Series Visualizations** existed even before the 1600s. An early example from around 1030 by al-Biruni shows the phases of the moon in orbit.\n",
        "\n",
        "* **Abstract & Thematic Graphics**: A critical development was the use of graphics to represent abstract, non-geographical data.\n",
        "    * **William Playfair (late 1700s)** was a pioneer, inventing the line graph and bar chart to show economic data like national debt and trade balances over time.\n",
        "    * **Charles Joseph Minard's Map (1869)** of Napoleon's march on Moscow is considered a masterpiece. It brilliantly visualizes six variables on a 2D surface: army size (via the width of the line), geographical location, direction of movement, temperature during the retreat, and time.\n",
        "    * **Florence Nightingale (1850s)** used a \"coxcomb\" diagram to show that far more soldiers died from preventable diseases in hospitals than from wounds on the battlefield, compelling sanitary reforms.\n",
        "\n",
        "* **Modern Visualization & the Role of Statistics**:\n",
        "    * **Anscombe's Quartet (1973)** is a critical example demonstrating the need for visualization. It comprises four datasets that have nearly identical simple statistical properties (mean, variance, correlation, linear regression). However, when plotted, they reveal four very different structures. This proves that relying on statistics alone can be highly misleading.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1.3 Relationship between Visualization and Other Fields**\n",
        "\n",
        "* **Visualization vs. Computer Graphics**:\n",
        "    * **Computer Graphics (CG)** is the set of tools and techniques used to create images. Its primary focus is often on synthesizing realistic images of 3D objects for art, entertainment, and games. It is the **underpinning** of visualization.\n",
        "    * **Visualization** uses CG as a medium, but its goal is the **effective communication of data**, not visual realism. It encompasses aspects from other fields like HCI, psychology, statistics, and data mining. The underlying models (data vs. geometric objects) and goals (insight vs. realism) are different.\n",
        "\n",
        "* **Scientific Visualization vs. Information Visualization**:\n",
        "    * Historically, a distinction was made between **scientific visualization** (for physical data like MRI scans or fluid flow) and **information visualization** (for abstract data like financial records or networks).\n",
        "    * This book does not make a strong distinction, treating them as allied fields that both provide representations of data.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1.4 The Visualization Process**\n",
        "\n",
        "Creating a visualization is a multi-stage process, often described as a pipeline. User interaction can occur at any stage.\n",
        "\n",
        "* **The Core Visualization Pipeline**:\n",
        "    * **Data Modeling / Preprocessing**: Raw data is filtered, sampled, and structured into a usable format.\n",
        "    * **Data Selection**: A subset of the data is chosen for visualization (similar to clipping in graphics).\n",
        "    * **Data to Visual Mappings**: The heart of the process. Data values are mapped to graphical attributes like position, size, shape, and color.\n",
        "    * **View Transformations / Scene Parameters**: The user specifies non-data attributes like camera position, lighting, and color maps.\n",
        "    * **Rendering**: The computer generates the final image, including axes, keys, and annotations.\n",
        "\n",
        "* **Role of Perception**: The final stage is the human. A visualization's effectiveness depends on the abilities and limitations of the human visual system.\n",
        "    * **Preattentive Processing**: The visual system can rapidly and accurately detect a limited set of visual properties in parallel, without focused attention. These include differences in color, shape, orientation, and size.\n",
        "    * **Attentive Processing**: Is a slower, serial process that requires focused attention to understand more complex conjunctions of features.\n",
        "    Effective visualizations harness preattentive features to draw the user's attention to important aspects of the data.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1.5 The Role of Cognition**\n",
        "\n",
        "Cognition goes beyond perception. It involves understanding, remembering, and reasoning about what is seen. A more complete model of the visualization process includes not just the computer's rendering pipeline but also the human's cognitive pipeline, where perception feeds into cognition to produce knowledge.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1.6 Pseudocode Conventions**\n",
        "\n",
        "This section of the book establishes the conventions used for presenting algorithms, ensuring clarity and consistency. The pseudocode is designed to convey the core logic of an algorithm without being tied to a specific programming language, graphics library, or detailed data management.\n",
        "\n",
        "The book assumes the following global variables and functions are available in the environment where the pseudocode runs:\n",
        "\n",
        "  * **`data`**: Represents the working data table, which is assumed to contain only numeric values.\n",
        "  * **`m`**: The number of dimensions (columns) in the working data table.\n",
        "  * **`n`**: The number of records (rows) in the working data table.\n",
        "  * **`NORMALIZE(record, dimension, min, max)`**: A function that maps a data value to a specified range. If `min` and `max` are not provided, it maps the value to the range between 0 and . The book notes this can be adapted for different normalization types, such as linear or logarithmic.\n",
        "  * **`COLOR(color)`**: A function that sets the current drawing color for the graphics environment.\n",
        "  * **`MAPCOLOR(record, dimension)`**: A function that applies a global color map to the normalized value of a given record and dimension to set the drawing color.\n",
        "  * **Drawing Primitives**:\n",
        "      * **`CIRCLE(x, y, radius)`**: Fills a circle at a given location with a specific radius using the current color setting.\n",
        "      * **`POLYLINE(xs, ys)`**: Draws a series of connected line segments based on arrays of x and y coordinates.\n",
        "      * **`POLYGON(xs, ys)`**: Fills a polygon defined by arrays of x and y coordinates with the current color setting.\n",
        "  * **Geographic and Graph Functions**:\n",
        "      * The book also assumes functions for handling geographic data (like `GETLATITUDES`) and graph data (`GETCONNECTIONS`) exist when needed.\n",
        "\n",
        "-----\n",
        "\n",
        "#### **1.7 The Scatterplot**\n",
        "\n",
        "The scatterplot is one of the most fundamental and widely used visualization techniques, based on the Cartesian coordinate system.\n",
        "\n",
        "* **Process**: It maps data from two dimensions of a dataset to the x and y axes of a plot. Additional data dimensions can be mapped to other visual attributes like the color, size, and shape of the plotted points (or glyphs).\n",
        "* **Example from the Book**: The text uses a 2004 vehicle dataset to show the exploratory power of scatterplots.\n",
        "    * Plotting horsepower vs. city MPG for just Toyota vehicles immediately reveals a clear, nearly linear inverse relationship.\n",
        "    * This generates a hypothesis: for foreign cars, higher horsepower means lower MPG.\n",
        "    * Testing this on Kia vehicles shows a similar pattern, confirming the hypothesis.\n",
        "    * Testing it again on Lexus vehicles shows that the relationship is not as simple, leading to refinement and further exploration. This demonstrates the cycle of exploration and hypothesis testing that visualization enables.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1.8 The Role of the User**\n",
        "\n",
        "The user's purpose or goal is central to the design of a visualization. Visualizations can be categorized by their intended role:\n",
        "\n",
        "* **Exploration**: The user has a dataset and wants to examine it to find interesting, unknown patterns, features, or outliers.\n",
        "* **Confirmation**: The user already has a hypothesis about the data and uses the visualization to confirm or refute it.\n",
        "* **Presentation**: The user already knows the story in the data and is creating a static visualization to communicate that story clearly to an audience.\n",
        "* **Interactive Presentation**: A presentation that allows the end-user some degree of exploration within a guided framework, common on the web.\n",
        "\n",
        "***\n",
        "\n",
        "### **Chapter 2: Data Foundations**\n",
        "\n",
        "Since every visualization begins with data, this chapter examines the fundamental characteristics of data, its structure, and the common preprocessing steps required to make it suitable for visualization.\n",
        "\n",
        "A typical dataset consists of a list of *n* records ($r_1, r_2, ..., r_n$), where each record $r_i$ is made up of *m* variables or observations ($v_1, v_2, ..., v_m$). Variables can be classified as **independent** (not affected by other variables, like time) or **dependent** (affected by other variables, like temperature at a given time and location).\n",
        "\n",
        "---\n",
        "\n",
        "#### **2.1 Types of Data**\n",
        "\n",
        "Data can be categorized based on its measurement type (ordinal vs. nominal) and its mathematical scale.\n",
        "\n",
        "* **Ordinal (Numeric) Data**: This data consists of numeric values.\n",
        "    * **Binary**: Can only take values of 0 or 1.\n",
        "    * **Discrete**: Can only take integer values or values from a specific subset (e.g., (2, 4, 6)).\n",
        "    * **Continuous**: Represents real values within a given interval (e.g., [0, 5]).\n",
        "\n",
        "* **Nominal (Non-numeric) Data**: This data consists of non-numeric values.\n",
        "    * **Categorical**: A value selected from a finite list of possibilities (e.g., 'red', 'blue', 'green').\n",
        "    * **Ranked**: A categorical variable that has a clear, implied ordering (e.g., 'small', 'medium', 'large').\n",
        "    * **Arbitrary**: A variable with a potentially infinite range of values with no implied ordering (e.g., street addresses).\n",
        "\n",
        "* **Data Scale Attributes**: These attributes help further classify variables.\n",
        "    * **Ordering relation**: A property that allows the data to be sorted. All ordinal and ranked nominal variables have this property.\n",
        "    * **Distance metric**: A property that allows the distance between different records to be computed. This is present in all ordinal variables but generally not in nominal ones.\n",
        "    * **Existence of absolute zero**: A property where a variable has a fixed lowest possible value. This helps differentiate types of ordinal data; for example, 'weight' has an absolute zero, while 'bank balance' (which can be negative) does not.\n",
        "\n",
        "The type of data determines the valid mathematical operations. For example, comparison operators (`<`, `>`) can only be applied to ranked nominal and ordinal data.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2.2 Structure within and between Records**\n",
        "\n",
        "Data sets have structure, both in their representation (syntax) and in the interrelationships between data items (semantics).\n",
        "\n",
        "* **2.2.1 Scalars, Vectors, and Tensors**\n",
        "    * A **scalar** is a single number in a data record, like the cost of an item.\n",
        "    * A **vector** is a composite item made of multiple values, like a 2D position `(x, y)` or a 3-component color `(R, G, B)`.\n",
        "    * A **tensor** is a more general structure represented as an array or matrix. A scalar is a tensor of rank 0, and a vector is a tensor of rank 1. A rank M tensor in a D-dimensional space requires $D^M$ data values.\n",
        "\n",
        "* **2.2.2 Geometry and Grids**\n",
        "    * Geometric structure can be **explicit**, where coordinates are included for each data record (e.g., longitude and latitude for temperature sensors).\n",
        "    * It can also be **implied**, where a grid structure is assumed, and successive records are located at successive grid locations (e.g., pixels in an image). Such grids can be uniform or **nonuniform/irregular**, where data is computed more densely in areas of high importance.\n",
        "\n",
        "* **2.2.3 Other Forms of Structure**\n",
        "    * **Timestamp**: An important attribute for time-oriented data, which can be relative or absolute.\n",
        "    * **Topology**: Describes how data records are connected. For example, vertices on a surface are connected by edges. This connectivity information is essential for processes like resampling and interpolation.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2.3 Data Preprocessing**\n",
        "\n",
        "Before visualization, raw data often needs to be cleaned, transformed, or reduced.\n",
        "\n",
        "* **2.3.1 Metadata and Statistics**: Information *about* the data, known as **metadata**, provides crucial context like data formats, units, and what value signifies a missing entry. Basic statistics like the **mean** ($\\mu = \\frac{1}{n} \\sum_{i=0}^{n-1} x_i$) and **standard deviation** ($\\sigma = \\sqrt{\\frac{1}{n}\\sum(x_i - \\mu)^2}$) can provide useful insights and help detect outliers.\n",
        "\n",
        "* **2.3.2 Missing Values and Data Cleansing**: Real-world data is often incomplete or erroneous. Common strategies to handle this include:\n",
        "    * **Discard the record**: The simplest but potentially most costly in terms of information loss.\n",
        "    * **Assign a sentinel value**: Use a designated value (e.g., -999) to mark missing entries, making them clearly visible in a visualization.\n",
        "    * **Assign the average value**: Replaces the missing entry with the mean for that variable. This minimally affects statistics but may mask interesting outliers.\n",
        "    * **Assign value based on nearest neighbor**: Find the most similar record based on other variables and use its value.\n",
        "    * **Compute a substitute value**: Use advanced statistical methods, a process known as **imputation**, to generate a high-confidence substitute value.\n",
        "\n",
        "* **2.3.3 Normalization**: It is about scaling your data. It takes a set of values, like test scores from 50 to 100, and shrinks them to a fixed range, most commonly between 0 and 1. This is crucial for things like using data to determine color; if your data ranges from 1 to 1,000,000, you can't easily map that to a color gradient unless you normalize it first.\n",
        "    * **Linear Normalization**: It is the simplest method, using a direct calculation to map the minimum value to 0 and the maximum to 1, with all other values scaled proportionally in between.\n",
        "           $d_{normalized} = (d_{original} - d_{min}) / (d_{max} - d_{min})$.\n",
        "    * **Non-linear Normalization**:like using a square root or logarithm, is more appropriate for data that is **skewed**. This helps to spread out values that are tightly clustered at one end of the scale, making their differences more visible in the visualization. ($d_{sqrt-normalized} = (\\sqrt{d_{original}} - \\sqrt{d_{min}}) / (\\sqrt{d_{max}} - \\sqrt{d_{min}})$) or logarithmic ($d_{log-normalized} = (\\log d_{original} - \\log d_{min}) / (\\log d_{max} - \\log d_{min})$) may be more appropriate.\n",
        "\n",
        "* **2.3.4 Segmentation**: It involves dividing a dataset into distinct, classified regions. Think of it like a find and separate operation. For example, in a medical scan, segmentation can automatically identify and outline different tissue types (like bone, muscle, and fat) so they can be analyzed individually. The **split-and-merge** process is a common way to refine these segments, ensuring they are as accurate as possible.\n",
        "\n",
        "* **2.3.5 Sampling and Subsetting**: This technique is all about changing the resolution of a dataset.\n",
        "    * **Interpolation**: It is used to fill in missing data points or increase the resolution. For instance, if you have temperature readings for every other city, interpolation can estimate the temperature in the cities in between.\n",
        "        * **Linear Interpolation**: Estimates a value at point C between points A and B using the formula: $d_C = d_A + (d_B - d_A) \\times (x_C - x_A) / (x_B - x_A)$.\n",
        "        * **Bilinear Interpolation**: Extends this to 2D by first interpolating horizontally between four grid points and then interpolating vertically using those results.\n",
        "        * **Nonlinear Interpolation**:(using curves like splines) creates a smoother result and is useful for things like drawing a smooth path between data points.\n",
        "\n",
        "* **2.3.6 Dimension Reduction**: Techniques to reduce the dimensionality of data for visualization while preserving as much information as possible.\n",
        "    * **Principal Component Analysis (PCA)**: A popular method that computes new dimensions (principal components) which are linear combinations of the originals. These new dimensions are sorted by how much they contribute to explaining the variance of the data.\n",
        "    * **Multidimensional Scaling (MDS)**: It is another approach that preserves the **distances** between data points. It rearranges points in a lower-dimensional space to best reflect their original relationships, so points that were close together in the high-dimensional data remain close in the visualization.\n",
        "\n",
        "* **2.3.7 Mapping Nominal Dimensions to Numbers**: For non-numeric categories (like car types or colors), you need to map them to visual attributes. While **color** and **shape** are often used, more complex statistical methods like **correspondence analysis** can map these categories to a numeric scale based on their relationships with other data, providing a more meaningful representation.\n",
        "\n",
        "* **2.3.8 Aggregation and Summarization**: When dealing with massive datasets, displaying every single data point can create visual clutter. **Aggregation** is the process of grouping similar data points and representing the group with a summary statistic, like a count or an average. This provides a clear overview without overwhelming the viewer. For example, instead of showing every single sale, you might aggregate them to show the total sales per month.\n",
        "\n",
        "* **2.3.9 Smoothing and Filtering**: This is a process to reduce **noise** and create a cleaner, more generalized visualization. It works by taking a **weighted average** of a data point's neighbors. For example, in a time-series graph with a lot of jitter, applying smoothing can reveal the underlying trend by blurring out the small, random fluctuations. This is often done using a mathematical technique called **convolution**. A simple 1D smoothing formula is $p'_{i} = \\frac{p_{i-1}}{4} + \\frac{p_{i}}{2} + \\frac{p_{i+1}}{4}$.\n",
        "\n",
        "* **2.3.10 Raster-to-Vector Conversion**: This technique is used to extract clean shapes from pixel-based data. **Raster** data is a grid of pixels (like a JPEG image), while **vector** data is a collection of lines and polygons (like a CAD drawing). This conversion is useful for things like outlining a region in a satellite image or converting a scanned blueprint into a digital, editable drawing. Techniques include **thresholding**, **region-growing**, **boundary-detection** (using convolution), and **thinning**. Methods like **thresholding** or **boundary detection** are used to identify the edges of objects and turn them into vectors.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOaLhyZFU8ymhEC8ZB8wa2U",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
